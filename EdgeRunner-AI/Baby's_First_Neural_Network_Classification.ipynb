{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baby's First Neural Network: Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwgb93/SIAM-Neural-Nets/blob/main/EdgeRunner-AI/Baby's_First_Neural_Network_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcYiUND0k0a"
      },
      "source": [
        "#My First Neural Network: Convergence\n",
        "\n",
        "In this notebook, we'll be creating a simple neural network and using it to classify handwritten digits. This is from the popular MNIST data set consisting of 60,000 digits pre-split into a training and testing set.\n",
        "\n",
        "First: some bookkeeping. We'll import a bunch of packages that let this all work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whUShDuM06gn"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kql2bjeEyJLg"
      },
      "source": [
        "Next we'll download the data we are going to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaWv1QDwyMUp"
      },
      "source": [
        "# the data is already split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Each pixel is a value from 0 to 255, so we normalize it to 0 to 1.\n",
        "# This is optional, and generally speeds up training. You could also do -1 to 1. Try it and see what works best!\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqEsEVVxzGTc"
      },
      "source": [
        "Let's look at a few of the images so we know what we're dealing with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9nRsY4zK0W"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize = (16,5))\n",
        "fig.add_subplot(1,3,1)\n",
        "plt.imshow(x_train[0], cmap=\"gray\")\n",
        "plt.title(\"Label is \"+ str(y_train[0]), fontsize = 18)\n",
        "fig.add_subplot(1,3,2)\n",
        "plt.imshow(x_train[1], cmap=\"gray\")\n",
        "plt.title(\"Label is \"+ str(y_train[1]), fontsize = 18)\n",
        "fig.add_subplot(1,3,3)\n",
        "plt.imshow(x_train[2], cmap=\"gray\")\n",
        "plt.title(\"Label is \"+ str(y_train[2]), fontsize = 18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQVNW3a4g5m"
      },
      "source": [
        "Finally, let's build our first neural network!\n",
        "\n",
        "We'll start with the input layer. Since we have a 28x28 image, we need 784 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egfEAbZp4o1e"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "  keras.Input(shape=(28, 28)),\n",
        "  # we start with our input layer\n",
        "  keras.layers.Flatten(),\n",
        "\n",
        "  # next, we'll add a hidden layer. Maybe 128 neurons? Why not\n",
        "  keras.layers.Dense(128, activation='sigmoid'),\n",
        "\n",
        "  # finally, we need an output layer, with\n",
        "  keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ug-hL497PYD"
      },
      "source": [
        "Now, let's train the neural network!\n",
        "\n",
        "We'll start by going through the entire dataset 5 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBbGbg96-eW"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "model.evaluate(x_test, y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6fT02rc7p1a"
      },
      "source": [
        "Not bad! 90% accuracy in less than ~20~ 45 seconds! (I can't believe they nuked Colab).\n",
        "How can we do better?\n",
        "\n",
        "The obvious step is to just keep training for longer.\n",
        "\n",
        "Try going for 20 epochs and see if it does any better.\n",
        "\n",
        "What if we change the activation function?\n",
        "\n",
        "Sigmoid:\n",
        "\n",
        "Tanh:\n",
        "\n",
        "ReLU:\n",
        "\n",
        "What you might have started to notice though is that our training accuracy is  better than our testing accuracy.\n",
        "\n",
        "We've looked at the training set so many times that we're starting to learn ONLY those images.\n",
        "\n",
        "This is known as ***overfitting***, and we generally want to avoid it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKKJ9w6H-ln8"
      },
      "source": [
        "Techniques we can use to avoid overfitting:\n",
        "\n",
        "\n",
        "*   Reduce the number of parameters in our network\n",
        "*   Change the structure of our network\n",
        "*   Train for fewer epochs\n",
        "*   Don't use all the neurons at once\n",
        "\n",
        "Let's give these a try!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAczpY-hgpco"
      },
      "source": [
        "plt.bar(np.arange(0,10,1), model.predict(x_train[0][np.newaxis,:])[0])\n",
        "plt.title (\"Proabibility for each integer\", fontsize = 16)\n",
        "plt.xticks(np.arange(0,10,1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqzfXq-jgyFJ"
      },
      "source": [
        "What if we have random noise?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG23H8eggwg7"
      },
      "source": [
        "my_noise = np.random.rand(784)\n",
        "plt.imshow(np.reshape(my_noise, (28,28)), cmap=\"gray\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EcYTKTlhD6m"
      },
      "source": [
        "plt.bar(np.arange(0,10,1), model.predict(my_noise.reshape(28,28)[np.newaxis,:])[0])\n",
        "plt.title (\"Proabibility for each integer\", fontsize = 18)\n",
        "plt.xticks(np.arange(0,10,1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPxpccNJEQQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}