{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baby's Third Neural Network: Autoencoders.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwgb93/SIAM-Neural-Nets/blob/main/EdgeRunner-AI/Baby's_Third_Neural_Network_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcYiUND0k0a"
      },
      "source": [
        "#My Third Neural Network: Autoencoders\n",
        "\n",
        "Convolutional neural nets are great, but they take a lot of time to run. One thing we can do reduce the dimension of our data from 28x28 down to a much smaller latent space.\n",
        "\n",
        "We can then classify numbers in their latent space, and recover an approximation of the original data using a decoder.\n",
        "\n",
        "This is the basis of how some Generative Adversarial Networks (GANs) work, for synthesizing new data.\n",
        "\n",
        "This is one way that embeddings can be trained (although they aren't the only way).\n",
        "\n",
        "Once again: bookkeeping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whUShDuM06gn"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import time\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kql2bjeEyJLg"
      },
      "source": [
        "Next we'll download the data we are going to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaWv1QDwyMUp"
      },
      "source": [
        "# the data is already split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Each pixel is a value from 0 to 255, so we normalize it to 0 to 1.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqEsEVVxzGTc"
      },
      "source": [
        "An autoencoder is made of two parts: an _encoder_ and a _decoder_. The function below ```create_autoencoders()``` returns the following parts as separate models:\n",
        "\n",
        "- The encoder\n",
        "- the decoder\n",
        "- the complete model, when the encoder and decoder are joined in one model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9nRsY4zK0W"
      },
      "source": [
        "def create_autoencoders(feature_layer_dim = 16):\n",
        "  # This is yet another way Keras lets us define a neural network.\n",
        "  input_img = Input(shape = (784,), name = 'Input_Layer')\n",
        "  # The layer encoded has a dimension equal to feature_layer_dim and contains\n",
        "  # the encoded input (therefore the name)\n",
        "  encoded = Dense(feature_layer_dim, activation = 'relu', name = 'Encoded_Features')(input_img)\n",
        "  decoded = Dense(784, activation = 'sigmoid', name = 'Decoded_Input')(encoded)\n",
        "\n",
        "  autoencoder = Model(input_img, decoded)\n",
        "  encoder = Model(input_img, encoded)\n",
        "\n",
        "  encoded_input = Input(shape = (feature_layer_dim,))\n",
        "  decoder = autoencoder.layers[-1]\n",
        "  decoder = Model(encoded_input, decoder(encoded_input))\n",
        "\n",
        "  return autoencoder, encoder, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn5-pLgpOcyK"
      },
      "source": [
        "So let's create our first autoencoder, with 8 features in our latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfX9US21OZHr"
      },
      "source": [
        "autoencoder, encoder, decoder = create_autoencoders(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hudh7vfTOZU5"
      },
      "source": [
        "keras.utils.plot_model(autoencoder, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQVNW3a4g5m"
      },
      "source": [
        "Autoencoders are like any other neural network we've been dealing with\n",
        "\n",
        " **define** (done) -> **compile** -> **fit**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egfEAbZp4o1e"
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ug-hL497PYD"
      },
      "source": [
        "Now, let's train our autoencoder.\n",
        "\n",
        "All we're doing is going through the network and converting each image from a vector of length 784 to a vector of length 8, then trying to recreate the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBbGbg96-eW"
      },
      "source": [
        "autoencoder.fit(x_train, x_train, epochs=30, batch_size=256, shuffle=True, validation_data=(x_test, x_test), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6fT02rc7p1a"
      },
      "source": [
        "Let's encode and decode the test images now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljiu7dvF7kOX"
      },
      "source": [
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKKJ9w6H-ln8"
      },
      "source": [
        "#Image reconstruction\n",
        "\n",
        "Now that we've come up with an encoding scheme, let's try and reconstruct the images.\n",
        "\n",
        "Since our latent space is so small, we can actaully visualize the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vbgN13B9Zqn"
      },
      "source": [
        "n = 10 # how many digits we will display\n",
        "\n",
        "fig = plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display embedding\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(encoded_imgs[i].reshape(4, 2)) # Change these to match your dimension.\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PbUHVX3ROau"
      },
      "source": [
        "Not great. What happens if we increase the number of latent features?\n",
        "\n",
        "Let's try n = 64.\n",
        "\n",
        "What about n = 16?\n",
        "\n",
        "As we can see, it only takes a small latent space to generate realiztic reconstructions.\n",
        "\n",
        "Can we use these encoded latent space to make predictions? Let's find out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpEH8T8nQW0m"
      },
      "source": [
        "encoded_train = encoder.predict(x_train)\n",
        "encoded_test = encoder.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mqS5Yx_ORQ4"
      },
      "source": [
        "This is the same as the first neural network, but where we input encoded images instead of all 784 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkkXybjStvT"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(Input(shape=(len(encoded_train[0]),)))\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(10, activation=\"softplus\"))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV9KLSrrT3lc"
      },
      "source": [
        "model.fit(encoded_train, y_train, epochs=20)\n",
        "\n",
        "model.evaluate(encoded_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeNns-evVKww"
      },
      "source": [
        "I can't believe that actually worked. That's nearly the same accuracy we got the first time around, but on a network 10x smaller.\n",
        "\n",
        "This is usually used in applications like k-Nearest Neighbours, which can experience close to a 1000x speedup when using encoded data enstead of full data, with only minimal loss in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zywb1N2AaibZ"
      },
      "source": [
        "# Don't bother running this one. It takes more than 15 minutes.\n",
        "# Edit: I lied. They've optimized the hell out of this in the last 4 years.\n",
        "# Should only take 30s now. So take everything that follows with a grain of salt.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "start = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(x_train, y_train)\n",
        "# accuracy on X_test\n",
        "print(\"Accuracy:\", 100 * knn.score(x_test, y_test) , \"%\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time:\", end - start, \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MMTMVgdFOv3"
      },
      "source": [
        "start = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(encoded_train, y_train)\n",
        "# accuracy on X_test\n",
        "print(\"Accuracy:\", 100 * knn.score(encoded_test, y_test) , \"%\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time:\", end - start, \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4iQBfJVFO7z"
      },
      "source": [
        "We get close to 1000x faster with n = 8, but lose a lot of accuracy.\n",
        "\n",
        "n = 64 is still 10x faster, with almost no loss of accuracy though.\n",
        "\n",
        "n = 16 is a sweet spot, more than 100x faster, with only 1.5% loss of accuracy\n",
        "\n",
        "But what if we want to generate images on our own?\n",
        "\n",
        "#Bonus Section: GANs\n",
        "\n",
        "#Generative Adversarial Networks\n",
        "\n",
        "The following code was provided in MA797: Applications of Machine Learning, which is I took in Fall 2019.\n",
        "\n",
        "(I modified the generator and discriminator)\n",
        "\n",
        "Note: This code mostly sucks and takes froever to run. Feel free to modify it further (uncomment some parts, maybe?) to try and make it good.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDhJWvRlPfID"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') #for using matplotlib remotely on the GPU workstation\n",
        "from keras.datasets import mnist #note that we are using the mnist data\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "#define the DCGAN class\n",
        "class DCGAN():\n",
        "    #define the initial default parameters\n",
        "    #to change these defaults, just do it when you call the DCGAN class\n",
        "    #for example, dcganrun = DCGAN(img_rows=64,img_cols=64,channels=3)\n",
        "    def __init__(self,img_rows=28,\n",
        "                 img_cols=28,\n",
        "                 channels=1,\n",
        "                 latent_dim=100):\n",
        "        # Input shape and channels\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channels = channels\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = latent_dim\n",
        "        #Choose the optimizer\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        #The following 30 lines or so make the DCGAN computational graph\n",
        "        #It is composed of a random input z that goes into a generator.\n",
        "        #The output of the generator is then the input to the discriminator.\n",
        "        #The combined model only trains the generator\n",
        "\n",
        "        # Build and compile the discriminator, see function build_discriminator() below\n",
        "        #Note that the discriminator object is a keras model, so you can\n",
        "        #call functions that you would normally use for a keras model\n",
        "        #such as .compile(), or .trainable, see keras documentation\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        # We will make the combined model about 10 lines down, and\n",
        "        # when we do it we will want to fix the disciminator weights.\n",
        "        # Note that this is how we inform the generator to make realistic images.\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid) #note that, at this point:\n",
        "                                        #valid = disciminator(generator(z))\n",
        "        #compile the combined model\n",
        "        self.model_checkpoint = ModelCheckpoint('saved_model/combinedmodel.keras', monitor='accuracy', save_best_only=True, save_freq=\"epoch\")\n",
        "        self.callbacks_list=[self.model_checkpoint]\n",
        "\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
        "        self.combined.summary()\n",
        "\n",
        "    #the function to build the generator as a sequential model\n",
        "    #it takes as input the random noise vector\n",
        "    #then uses upsampling followed by Conv2d to upsample\n",
        "    #note that this was found to improve results over just using Conv2dtranspose\n",
        "    #also note the use of tanh in the last layer and batchnorm after the conv2ds,\n",
        "    #except for the last layer\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3,padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    #build the disciminator\n",
        "    #note the use of leakyReLU and dropout\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\")) #output is 14x14\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25)) #I found a suggestion to use dropout in the disciminator\n",
        "                                 #to prevent it from learning too quickly for the generator to learn anything\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\")) #output is 7x7\n",
        "        #model.add(ZeroPadding2D(padding=((0,1),(0,1)))) #this pads by 1 on the bottom and right\n",
        "                                                        #to make it an 8x8\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\")) #output is 4x4\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        #model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\")) #output is 4x4\n",
        "        #model.add(BatchNormalization(momentum=0.8))\n",
        "        #model.add(LeakyReLU(alpha=0.2))\n",
        "        #model.add(Dropout(0.25))\n",
        "        model.add(Flatten()) #flatten into a vector\n",
        "        model.add(Dense(1, activation='sigmoid')) #output is a single number between 0 and 1\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    #function to train the model, default batch size is 128\n",
        "    def train(self, epochs, batch_size=128, save_interval=1000):\n",
        "\n",
        "        # Load the dataset\n",
        "        (X_train, _), (_, _) = mnist.load_data() #mnist has its own loading function, nice!\n",
        "\n",
        "        # Rescale -1 to 1, note that 255/127.5=2, then subtract 1 to get a max of 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3) #convolutional layers expect 4d tensors in tensorflow\n",
        "\n",
        "        # Adversarial ground truths, i.e., labels\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample N(0,1) noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise) #note that since we are generating fake data from the generator, we don't effect the disciminator weights\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid) #first half of GAN loss, corresponds to real images\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)#second half of GAN loss, corresponds to fake images\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) #total GAN loss\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.fit(noise, valid, callbacks=self.callbacks_list)\n",
        "            #note that you use valid labels here, since we want these fake images to look real\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss.history['loss'][0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if (epoch + 1) % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "\n",
        "    #function for saving images\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5 #number of rows and columns for the plot of fake images, so 25 total fake images\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim)) #generate the random noise\n",
        "        gen_imgs = self.generator.predict(noise) #pass the noise through the trained generator\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "        #plot images in 5x5 plot\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(\"/content/images\", exist_ok=True)\n",
        "        fig.savefig(\"/content/images/mnist_%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEVCJ20glQ11"
      },
      "source": [
        "dcgan = DCGAN() #make a DCGAN class object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2ZpUGyNlSNG"
      },
      "source": [
        "dcgan.train(epochs=20000, batch_size=32, save_interval=1000) #train it and save images every 500 epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUmX5MjFuyZX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}